{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actividad: Concurrencia y ayncio\n",
    "\n",
    "[asyncio](https://docs.python.org/3/library/asyncio.html) es una biblioteca de Python que nos permite ejecutar código utilizando un modelo de programación asíncrono. Esto nos permite manejar múltiples operaciones de E/S a la vez, al mismo tiempo que permite que nuestra aplicación siga respondiendo. \n",
    "\n",
    "Utiliza un modelo de concurrencia conocido como bucle de eventos de un solo subproceso (single-threaded event loop.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operaciones vinculadas a E/S y a la CPU\n",
    "\n",
    "Cuando nos referimos a una operación como vinculada a E/S o vinculada a CPU, nos referimos al factor limitante que impide que esa operación se ejecute más rápido. Esto significa que si aumentamos el rendimiento de aquello a lo que estaba vinculada la operación, esa operación se completaría en menos tiempo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operaciones vinculadas a E/S y CPU\n",
    "import requests\n",
    "\n",
    "respuestas = requests.get('https://www.example.com')\n",
    "\n",
    "items = respuestas.headers.items()\n",
    "\n",
    "cabeceras = [f'{key}: {header}' for key, header in items]\n",
    "# Completa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multitarea\n",
    "\n",
    "La multitarea está en todas partes en el mundo actual.  En esta sección se analizan dos tipos principales de multitarea: multitarea preventiva y multitarea cooperativa. \n",
    "\n",
    "**Multitarea preventiva**\n",
    "\n",
    "En este modelo, dejamos que el sistema operativo decida cómo cambiar entre qué trabajo se está ejecutando actualmente mediante un proceso llamado división de tiempo (time slicing). Cuando el sistema operativo cambia de trabajo, lo llamamos apropiación. \n",
    "\n",
    "El funcionamiento de este mecanismo  depende del propio sistema operativo. Se logra principalmente mediante el uso de múltiples subprocesos o múltiples procesos. \n",
    "\n",
    "**Multitarea cooperativa**\n",
    "\n",
    "En este modelo, en lugar de depender del sistema operativo para decidir cuándo cambiar entre qué trabajo se está ejecutando actualmente, codificamos explícitamente puntos en nuestra aplicación donde podemos permitir que se ejecuten otras tareas. Las tareas en nuestra aplicación operan en un modelo en el que cooperan, diciendo explícitamente: “Estoy pausando mi tarea por un tiempo; adelante y ejecute otras tareas”. \n",
    "\n",
    "\n",
    "asyncio utiliza la multitarea cooperativa para lograr la concurrencia.  \n",
    "\n",
    "La multitarea cooperativa tiene ventajas sobre la multitarea preventiva.\n",
    "\n",
    "- La multitarea cooperativa requiere menos recursos. \n",
    "- Granularidad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Procesos, subprocesos, multithreading y multiprocessing \n",
    "\n",
    "-  Un proceso es la ejecución de una aplicación que tiene un espacio de memoria al que otras aplicaciones no pueden acceder.\n",
    "- Los hilos pueden considerarse procesos más livianos. Además, son la construcción más pequeña que puede gestionar un sistema operativo. No tienen memoria propia como la tiene un proceso; en cambio, comparten la memoria del proceso que los creó. Los hilos están asociados con el proceso que los creó. Un proceso siempre tendrá al menos un hilo asociado, generalmente conocido como `hilo principal`. Un proceso también puede crear otros subprocesos, que se conocen más comúnmente como subprocesos de trabajo o en segundo plano. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "\n",
    "print(f'Proceso de Python ejecutandose con un ID: {os.getpid()}')\n",
    "\n",
    "total_threads = threading.active_count()\n",
    "nombre_thread = threading.current_thread().name\n",
    "\n",
    "print(f'Python esta corriendo {total_threads} thread(s) actualmente')\n",
    "print(f'El actual thread es {nombre_thread}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los procesos también pueden crear otros subprocesos que comparten la memoria del proceso principal. Estos subprocesos pueden realizar otros trabajos simultáneamente a través de lo que se conoce como **multithreading**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "\n",
    "def hello_thread():\n",
    "    print(f'Hola desde el thread {threading.current_thread()}!')\n",
    "\n",
    "\n",
    "hello_thread = threading.Thread(target=hello_thread)\n",
    "hello_thread.start()\n",
    "# completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El multithreaded no es la única forma en que podemos lograr la concurrencia. También podemos crear múltiples procesos para que trabajen simultáneamente para nosotros. Esto se conoce como **multiprocesamiento**. En el multiprocesamiento, un proceso padre crea uno o más procesos hijos que administra. Luego puede distribuir trabajo a los procesos hijos. \n",
    "\n",
    "Python nos proporciona el módulo `multiprocessing` para manejar esto. La API es similar a la del módulo de `threading`. Primero creamos un proceso con una función `target`. Luego, llamamos a su método `start` para ejecutarlo y finalmente a su método `join` para esperar a que complete su ejecución. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    " \n",
    " \n",
    "def hello_procesos():\n",
    "    print(f'Hola desde los procesos hijos {os.getpid()}!')\n",
    "if __name__ == '__main__':\n",
    "    hello_proceso = multiprocessing.Process(target=hello_procesos)\n",
    "    hello_proceso.start()\n",
    " \n",
    "    print(f'Hola desde el proceso padre {os.getpid()}')\n",
    " \n",
    "    hello_proceso.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multithreading y multiprocessing  pueden parecer soluciones mágicas para permitir la concurrencia con Python. Sin embargo, el poder de estos modelos de concurrencia se ve obstaculizado por un detalle de implementación de Python: el bloqueo global del intérprete (GIL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Por que existe el GIL?\n",
    "\n",
    "La respuesta está en cómo se gestiona la memoria en CPython. En CPython, la memoria se gestiona principalmente mediante un proceso conocido como [recuento de referencias](https://towardsdatascience.com/understanding-reference-counting-in-python-3894b71b5611) ( reference counting ). \n",
    "\n",
    "El recuento de referencias funciona al realizar un seguimiento de quién necesita actualmente acceso a un objeto de Python en particular, como un número entero, un diccionario o una lista. Un recuento de referencias es un número entero que registra cuántos lugares hacen referencia a ese objeto en particular. Cuando alguien ya no necesita ese objeto al que se hace referencia, el recuento de referencias disminuye y cuando alguien más lo necesita, se incrementa. Cuando el recuento de referencias llega a cero, nadie hace referencia al objeto y se puede eliminar de la memoria. \n",
    "\n",
    "El conflicto con los subprocesos surge porque la implementación en CPython no es segura para subprocesos. Cuando decimos que CPython no es seguro para subprocesos, queremos decir que si dos o más subprocesos modifican una variable compartida, esa variable puede terminar en un estado inesperado. Este estado inesperado depende del orden en que los subprocesos acceden a la variable, lo que comúnmente se conoce como **condición de carrera**. Las condiciones de carrera pueden surgir cuando dos subprocesos necesitan hacer referencia a un objeto Python al mismo tiempo.  \n",
    "\n",
    "Para resolver condiciones de carrera como esta, se deben usar mecanismos de sincronización como semáforos, mutexes o bloqueos (locks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para demostrar el efecto del GIL en la programación multithreaded, examinemos la tarea que requiere un uso intensivo de la CPU de calcular el enésimo número de la secuencia de Fibonacci. Usaremos una implementación bastante lenta del algoritmo para demostrar una operación que requiere mucho tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_fib(numero: int) -> None:\n",
    "    def fib(n: int) -> int:\n",
    "        if n == 1:\n",
    "            return 0\n",
    "        elif n == 2:\n",
    "            return 1\n",
    "        else:\n",
    "            return fib(n - 1) + fib(n - 2)\n",
    "\n",
    "    print(f'fib({numero}) is {fib(numero)}')\n",
    "\n",
    "\n",
    "def fibs_no_threading():\n",
    "    print_fib(40)\n",
    "    print_fib(41)\n",
    "\n",
    "\n",
    "inicio = time.time()\n",
    "\n",
    "fibs_no_threading()\n",
    "\n",
    "final = time.time()\n",
    "\n",
    "print(f'Completado en {final - inicio:.4f} segundos.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta implementación utiliza recursividad y, en general, es un algoritmo relativamente lento que requiere un tiempo exponencial $O(2^N)$ para completarse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usamos multithreading en la secuencia Fibonacci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "def print_fib(numero: int) -> None:\n",
    "    def fib(n: int) -> int:\n",
    "        if n == 1:\n",
    "            return 0\n",
    "        elif n == 2:\n",
    "            return 1\n",
    "        else:\n",
    "            return fib(n - 1) + fib(n - 2)\n",
    "\n",
    "    print(f'fib({numero}) is {fib(numero)}')\n",
    "\n",
    "\n",
    "def fibs_con_threads():\n",
    "    thread_40 = threading.Thread(target=print_fib, args=(40,))\n",
    "    thread_41= threading.Thread(target=print_fib, args=(41,))\n",
    "\n",
    "    thread_40.start()\n",
    "    thread_41.start()\n",
    "\n",
    "    thread_40.join()\n",
    "    thread_41.join()\n",
    "\n",
    "\n",
    "inicio_threads = time.time()\n",
    "\n",
    "fibs_con_threads()\n",
    "\n",
    "fin_threads = time.time()\n",
    "\n",
    "print(f'Threads toman {fin_threads - inicio_threads:.4f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta última versión  tomó casi la misma cantidad de tiempo. De hecho, ¡fue incluso un poco más lento! (comprueba!) Esto se debe casi en su totalidad al GIL y a la sobrecarga de crear y administrar subprocesos. Si bien es cierto que los subprocesos se ejecutan concurrentemente, solo uno de ellos puede ejecutar código Python a la vez debido al bloqueo. Esto deja al otro subproceso en estado de espera hasta que se completa el primero, lo que niega por completo el valor de varios subprocesos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿La concurrencia en Python puede ocurrir alguna vez con los subprocesos?\n",
    "\n",
    " El GIL no se mantiene para siempre, de modo que no podamos utilizar múltiples subprocesos para nuestro beneficio.  \n",
    "\n",
    "El bloqueo global del intérprete se libera cuando ocurren operaciones de E/S. Esto nos permite emplear subprocesos para realizar trabajo concurrente en lo que respecta a E/S, pero no para el código Python vinculado a la CPU en sí (hay algunas excepciones notables que liberan el GIL para el trabajo vinculado a la CPU en ciertas circunstancias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "def leer_ejemplo() -> None:\n",
    "    respuesta = requests.get('https://www.example.com')\n",
    "    print(respuesta.status_code)\n",
    "\n",
    "    #completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos una base de referencia de cómo se ve una versión síncrona, podemos escribir una versión multithreaded para compararla.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import requests\n",
    "\n",
    "\n",
    "def leer_ejemplo() -> None:\n",
    "    respuesta= requests.get('https://www.example.com')\n",
    "    print(respuesta.status_code)\n",
    "\n",
    "\n",
    "thread_1 = threading.Thread(target=leer_ejemplo)\n",
    "thread_2 = threading.Thread(target=leer_ejemplo)\n",
    "\n",
    "inicio_thread = time.time()\n",
    "\n",
    "thread_1.start()\n",
    "thread_2.start()\n",
    "\n",
    "print('Todos los subprocesos corriendo!')\n",
    "\n",
    "thread_1.join()\n",
    "thread_2.join()\n",
    "\n",
    "final_thread = time.time()\n",
    "\n",
    "print(f'Ejecutar sincronicamente toma {final_thread - inicio_thread:.4f} segundos.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es aproximadamente dos veces más rápido que nuestra versión original que no usaba subprocesos, ya que ejecutamos las dos solicitudes aproximadamente al mismo tiempo! (comprueba)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### asyncio y el GIL\n",
    "\n",
    "asyncio aprovecha el hecho de que las operaciones de E/S liberan el GIL para brindarnos concurrencia, incluso con un solo subproceso. Cuando utilizamos asyncio creamos objetos llamados **corutinas**.\n",
    "\n",
    " Es importante tener en cuenta que asyncio no elude el GIL y todavía estamos sujetos a él. Si tenemos una tarea vinculada a la CPU, aún necesitamos usar múltiples procesos para ejecutarlos concurrentemente (lo que se puede hacer con el propio asyncio); de lo contrario, causaremos problemas de rendimiento en nuestra aplicación. Ahora que sabemos que es posible lograr concurrencia para E/S con un solo subproceso, profundicemos en los detalles de cómo funciona esto con sockets sin bloqueo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Qué es un socket?\n",
    "\n",
    "Un socket es una abstracción de bajo nivel para enviar y recibir datos a través de una red. Es la base de cómo se transfieren los datos hacia y desde los servidores. Los sockets admiten dos operaciones principales: enviar bytes y recibir bytes.\n",
    "\n",
    "Los sockets se bloquean de forma predeterminada. En pocas palabras, esto significa que cuando esperamos que un servidor responda con datos, detenemos nuestra aplicación o la bloqueamos hasta que tengamos datos para leer. Por lo tanto, nuestra aplicación deja de ejecutar cualquier otra tarea hasta que obtengamos datos del servidor, ocurra un error o se agote el tiempo de espera. \n",
    "\n",
    " A nivel de sistema operativo no necesitamos hacer este bloqueo. Los sockets pueden funcionar en modo sin bloqueo. En el modo sin bloqueo, cuando escribimos bytes en un socket, podemos simplemente activar y olvidar la escritura o lectura, y nuestra aplicación puede continuar realizando otras tareas. Posteriormente, podemos hacer que el sistema operativo nos diga que recibimos bytes y lo trate en ese momento. Esto permite que la aplicación haga cualquier cantidad de cosas mientras esperamos que nos lleguen los bytes.\n",
    "  \n",
    "\n",
    "En segundo plano, esto lo realizan algunos **sistemas de notificación de eventos** diferentes, según el sistema operativo que estemos ejecutando. asyncio es lo suficientemente abstracto como para cambiar entre los diferentes sistemas de notificación, dependiendo de cuál admita nuestro sistema operativo.\n",
    "\n",
    "\n",
    "Estos sistemas realizan un seguimiento de nuestros sockets sin bloqueo y nos notifican cuando están listos para que hagamos algo con ellos. Este sistema de notificación es la base de cómo asyncio puede lograr la concurrencia. En el modelo de concurrencia de asyncio, solo tenemos un hilo ejecutando Python en un momento dado. Cuando realizamos una operación de E/S, la entregamos al sistema de notificación de eventos de nuestro sistema operativo para que realice un seguimiento por nosotros. Una vez que hayamos realizado esta transferencia, nuestro hilo de Python podrá seguir ejecutando otro código de Python o agregar más sockets sin bloqueo para que el sistema operativo realice un seguimiento por nosotros. Cuando finaliza nuestra operación de E/S, \"despertamos\" la tarea que estaba esperando el resultado y luego procedemos a ejecutar cualquier otro código Python que vino después de esa operación de E/S. \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo funciona un bucle de eventos?\n",
    "\n",
    "Un bucle de eventos es la base de cada aplicación asyncio. Los bucles de eventos son un patrón de diseño bastante común en muchos sistemas y existen desde hace bastante tiempo. Si alguna vez usó JavaScript en un navegador para realizar una solicitud web asincrónica, creó una tarea en un bucle de eventos. Las aplicaciones GUI de Windows utilizan lo que se denomina bucles de mensajes detrás de escena como mecanismo principal para manejar eventos como la entrada del teclado, al tiempo que permiten que la interfaz de usuario se dibuje. \n",
    "\n",
    "El bucle de eventos más básico es extremadamente simple. Creamos una cola que contiene una lista de eventos o mensajes. Luego hacemos un bucle indefinido, procesando los mensajes uno a la vez a medida que entran en la cola. En Python, un bucle de eventos básico podría verse así: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "from collections import deque\n",
    "mensajes = deque()\n",
    "\n",
    "while True:\n",
    "    if mensajes:\n",
    "        mensajes = mensajes.pop()\n",
    "        proceso_mensajes(mensajes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En asyncio, el bucle de eventos mantiene una cola de tareas en lugar de mensajes. Las tareas son envoltorios de una corutina. Una corutina puede pausar la ejecución cuando llega a una operación vinculada a E/S y permitirá que el bucle de eventos ejecute otras tareas que no están esperando a que se completen las operaciones de E/S. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
